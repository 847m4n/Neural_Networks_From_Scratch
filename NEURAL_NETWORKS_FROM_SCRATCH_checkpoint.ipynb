{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to neural networks**\n",
        "\n",
        "Think of a mathematical function. We have an equation of the form f(x) = x*x, here if we input any number in the function, it will return the number multiplied by itself (Or square). \n",
        "In general a function can be looked at as a machine that does something to our input (Or entry). \n",
        "The anatomy of a neural network is simple. we have neurons in layers. for instance, the first layer may have,\n",
        "\n",
        "3 neurons stacked above each other (O1, O2 & O3).\n",
        "Each one of these neurons is a function of the form Oj = IjWji + Bi , we can have multiple inputs going into each neuron together. the j here is the input number. say we have 4 inputs, the equation would look like:\n",
        "\n",
        "\n",
        "O1 = I1W11 + I2W21 + I3W31 + I4W41 + B1\n",
        "\n",
        "O2 = I1W12 + I2W22 + I3W32 + I4W42 + B2\n",
        "\n",
        "O3 = I1W13 + I2W23 + I3W33 + I4W43 + B3\n",
        "\n",
        "\n",
        "O1 would read as, \n",
        "output 1 = Input 1 * Weight from input 1 to output 1 + input 2 * weight from input 2 to output 1 + input 3 * weight from input 3 to output 1 + input 4 * weight from input 4 to output 1. \n",
        "\n",
        "Note:\n",
        "1. Here output 1 is the 1st neuron in the first layer.\n",
        "2. The inputs here refer to components of a single entry.\n",
        "\n",
        "Therefore O1, O2 & O3 would make up the first layer, if we have a 2nd layer with 2 neurons. the outputs from the first layer would then become the inputs of the second layer.\n",
        "\n",
        "One question to have is:\n",
        "  what is W and C here, where does it come from?\n",
        "\n",
        "\n",
        "To understand this, first let's go back to our function example.\n",
        "in neural network the function is not really defined for us. what that means is that the equation part \"x*x\" won't be present. we will have mulitple inputs (multiple entries)  like 2,3,5,6,7 and  we'll be having corresponding outputs as 4,9,25,36,49. these neural networks will be given the inputs one by one, here there would be a single neuron giving the inputs and we will give the corresponding output. the neural network is such a machine that would throw out the equation 'X*X' for us. \n",
        "\n",
        "Coming back to W and B, these are randomly initialized numbers. After every input is given, it first goes through all the calculations from the first layer, then the second layer after which it reaches some final output(s). These initially would most of the times be wrong.  \n",
        "\n",
        "Remember we have a true output as well. inputs and the true outputs were the only things we had to begin with. the network output and the true outputs will be compared. and we will have an idea about how the network has done.  we will do this for several entries. and get a number associated with how good our network is doing, this number is called, 'loss'.\n",
        "\n",
        "After this we will try to tweak these weights and biases using calculus in such a way that the loss is minimum and makes our network behave in a way that its output would be same as the true outputs. this process of updating the weights and biases is called back-propagation.\n",
        "\n",
        "\n",
        "After we have given enough examples, we will have a set of weights and biases that would act like that 'equation'. If we have moe inputs where we don't have the outputs. this 'equation' or the combination of 'trained' weights and biases would give us the correct outputs associated with the input. or at least most of the times it would do so. this brings us to the accuracy of a 'model', this is usually in percentage, which tells us how many times out of 100 inputs would we have a correct output.\n",
        "\n",
        "\n",
        "This can be used for much more complex problems like cat-dog classifier. where after training our weights and biases from hundreds and thousands of examples. it will be able to recognize whether an image contains a cat or a dog. or detecting the letter from handwriting. there are several other applications of neural networks. but they generally work for structured data, which simply means that for every entry we have well defined components/attributes etc (these are the different inputs that go into the neurons of the first layer.)\n",
        "\n"
      ],
      "metadata": {
        "id": "c4L8WjW2q__B"
      },
      "id": "c4L8WjW2q__B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What we will cover**\n",
        "\n",
        "We will program the neurons, layers, activation functions, optimization back-propagation all from scratch in python, first we'll do it without using any 3rd party libraries and then we will use numpy.\n",
        "\n",
        "optimization means the loss function that we will be using and activation functions are those functions that are added to every single neuron in a layer to bring about non linearity. we will understand this in more details later. \n",
        "\n",
        "numpy stands for numerical python. it allows us to work with arrays which are like lists. the difference is that arrays work much faster than lists. there are also various different funtionality in numpy like broadcasting etc. we will look at these later as well.\n"
      ],
      "metadata": {
        "id": "A-jnLadzEa6O"
      },
      "id": "A-jnLadzEa6O"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**let's get started**\n",
        "\n",
        "In a fully connected neural network every neuron has a connection with every  single previous neuron. so let's say that we have 3 neurons that are feeding into this neuron that we are going to build. so they'll have some output which will become the input of this neuron.\n",
        "Please note that we are just making up the inputs & weights here, but in reality the inputs will be given according to some data that may come from sensors, databases or from previous neurons.\n",
        "Now, to have some control over the value of our output, we need weights and biases. since we cannot really change the input values. they will be randomly initialized first. which will then be tuned during the back-propagation. \n",
        "note that getting to the best possible values for the weights or biases for our purspose is reached iteratively. and we reach here through back-propagation and forward propagation done many times. this is how the 'learning' happens.\n",
        "The optimizer tunes the values of the weights and biases in an attempt to fit to the data."
      ],
      "metadata": {
        "id": "Px3YYEDQFknv"
      },
      "id": "Px3YYEDQFknv"
    },
    {
      "cell_type": "code",
      "source": [
        "#Coding a single neuron having 3 inputs from the previous layer\n",
        "\n",
        "inputs = [2.2,3.4,3] #Outputs from sensors neurons in a list\n",
        "weights = [1.3,4.3,5.6] #Every input for this new neuron will have its own weights associated with it\n",
        "bias = 5 #every unique neuron will also have its own unique bias\n",
        "\n",
        "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias #the linear calculations for our neuron 'output'. the brackets has the value in index of associated list\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0MauH3ZFJJC",
        "outputId": "ec049527-3c01-4fb9-c7f5-ef6abd1d6cb9"
      },
      "id": "Q0MauH3ZFJJC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's say that we have a layer of 3 neurons. they will have the same inputs from every neuron of the previous layer but the weights assigned will be different. and every neuron of our layer will have its own bias.\n",
        "We'll eventually use loops or numpy to make this a lot less messier. "
      ],
      "metadata": {
        "id": "zhy_oiXbOPqS"
      },
      "id": "zhy_oiXbOPqS"
    },
    {
      "cell_type": "code",
      "source": [
        "#coding a layer of 3 neurons having 4 inputs from the previous layer\n",
        "\n",
        "inputs = [2,1,3,4] # We will only have one set of inputs coming from previous layer\n",
        "\n",
        "weights1 = [1.2,3.3,5.4,6.2] #Set of weights that will multiply with inputs to make the first neuron in current layer\n",
        "weights2 = [3.2,5.5,4.9,2.3] #Set of weights that will multiply with inputs to make the second neuron in current layer\n",
        "weights3 = [5.2,1.8,2.4,2.2] #Set of weights that will multiply with inputs to make the third neuron in current layer\n",
        "\n",
        "bias1 = 2 # For the first neuron\n",
        "bias2 = 8 # For the second neuron\n",
        "bias3 = 4 # For the third neuron\n",
        "\n",
        "output = [inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1, \n",
        "          inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2, \n",
        "          inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] + bias3] #\n",
        "\n",
        "print(output) # see that we have 3 values each corresponding to values of our resulting neurons in layer."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NljUbzenFI3j",
        "outputId": "63aa7122-c82c-4bac-c63b-df6e5da79dab"
      },
      "id": "NljUbzenFI3j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[48.7, 43.8, 32.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q61p-tVKKn25"
      },
      "id": "Q61p-tVKKn25"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Lists, Arrays and matrices**\n",
        "\n",
        "Now we will use numpy, we will also take a look at the shape.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   A list is a 1-d array in numpy and a vector in mathematics with shape (n, ).\n",
        "*   A list of lists is a 2-d array in numpy and a matrix in mathematics with shape (n,m), arrays have to be homologous which means that they'll have to have the same size at each dimension. since we could not really say what the shape would be in such a case.\n",
        "\n",
        "\n",
        "*   A list of list of list is a 3-d array. or a 3-d matrix. of shape (n,m,k).\n",
        "\n",
        "\n",
        "Dot product of 2 vector result in a single scalar value\n",
        "\n",
        "Now that we have the terminology out of the way, let's simplify things."
      ],
      "metadata": {
        "id": "4DWLmEJGc1-l"
      },
      "id": "4DWLmEJGc1-l"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "inputs = [2,1,3,4]\n",
        "weights  = np.array([[1.2,3.3,5.4,6.2],[3.2,5.5,4.9,2.3],[5.2,1.8,2.4,2.2]]) # 2-d array or a list of list\n",
        "biases = [2,4,8]\n",
        "\n",
        "#First let's try using a for loop\n",
        "# we'll first start with an empty list, which will contain the output of our layer\n",
        "# Then we'll multiply every ith list of weights with the input and add 1st bias for the first neuron, second list of weights * inputs + 2nd bias for second neuron and so on..\n",
        "\n",
        "layer = []\n",
        "for weight_n , bias_n in zip(weights, biases): #zip takes the ith element of each input and zips them into a tuple, we are accessing the elements through the for loop\n",
        "  output = 0\n",
        "  for input_n, weight_N in zip(inputs,weight_n):\n",
        "    output += input_n*weight_N \n",
        "  output += bias_n\n",
        "  layer.append(output)\n",
        "\n",
        "print(layer)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ar9Yr1krclcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6dfd7ac-d527-48ae-e128-3de12632bb05"
      },
      "id": "Ar9Yr1krclcX",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[48.7, 39.8, 36.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we will take the dot product where each element of two matrices are multiplied element wise and then they are summed up. (matrix multiplication)"
      ],
      "metadata": {
        "id": "cC47gxmmIZu6"
      },
      "id": "cC47gxmmIZu6"
    },
    {
      "cell_type": "code",
      "source": [
        "#doing the same with numpy\n",
        "import numpy as np\n",
        "\n",
        "inputs = [1,2,3,4]\n",
        "weight = [5,6,7,8]\n",
        "bias = 5\n",
        "\n",
        "print(np.dot(weight,inputs)+bias) #here elements at index i in inputs and weights are being mulitplied and then all elements are added together along with the bias\n",
        "#note that here the inputs and weight both are vectors so the order in which we write them will not matter but it will when we use matrices\n",
        "\n",
        "\n",
        "inputs = [12,34,53,43]\n",
        "weights  = [[1.2,3.3,5.4,6.2],[3.2,5.5,4.9,2.3],[5.2,1.8,2.4,2.2]]\n",
        "bias = [2,5,7]\n",
        "\n",
        "output = np.dot(weights,inputs) + bias #We'll get shape error if input comes first, because we cannot multiply 4 elements with 3. bias is then added element index wise\n",
        "output\n"
      ],
      "metadata": {
        "id": "IgvCpKtZcljx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51a1a03c-bff8-4a29-cf35-f96daa1fa5cc"
      },
      "id": "IgvCpKtZcljx",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([681.4, 589. , 352.4])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will take in inputs in batches. which means we'll have more individual data/entries, having the same number of attributes. we are going to put it into the list of inputs. note, the number of neurons we have are the same, so the number of weights and biases won't change for them. (the values will just update later on)."
      ],
      "metadata": {
        "id": "duhWCH63QlUq"
      },
      "id": "duhWCH63QlUq"
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [[12,34,53,43],[34,53,23,74],[6,4,2,5]] #inputs is a matrix now\n",
        "weights  = [[1.2,3.3,5.4,6.2],[3.2,5.5,4.9,2.3],[5.2,1.8,2.4,2.2]]\n",
        "bias = [2,5,7]\n",
        "\n",
        "output = np.dot(inputs,np.array(weights).T) + bias #here we'll get a shape error if we simply take weights*inputs we need to take transpose of either one, here the order won't matter\n",
        "output"
      ],
      "metadata": {
        "id": "q-i-iW6Yclr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82345b7a-0d15-42d1-e8d7-fdc6b3eac051"
      },
      "id": "q-i-iW6Yclr8",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[681.4, 589. , 352.4],\n",
              "       [800.7, 688.2, 497.2],\n",
              "       [ 64.2,  67.5,  61.2]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Doing this for 2 layers we are going to take the output form the first layer as the input for the second layer\n",
        "\n",
        "inputs = [[1,2,3,2.5],[2.0,5.0,-1.0,2.0],[-1.5,2.7,3.3,-0.8]]\n",
        "weights = [[0.2,0.8,-0.5,1],[0.5,-0.91,0.26,-0.5],[-0.26,-0.27,0.17,0.87]]\n",
        "bias =[ 2,3,0.5]\n",
        "\n",
        "weights2 = [[0.1,-0.14,0.5],[-0.5,0.12,-0.33],[-0.44,0.73,-0.13]]\n",
        "bias2 =[ -1,2,-0.5]\n",
        "\n",
        "layer1_outputs = np.dot(inputs, np.array(weights).T)+bias\n",
        "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T)+bias2 #We take layer1 outputs as our input for this layer. and the different set of weights and biases\n",
        "\n",
        "print(layer2_outputs)\n",
        "\n"
      ],
      "metadata": {
        "id": "sebEhNKdcly3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ac0f0e7-2de2-4ef0-ebd5-306f5a8f930a"
      },
      "id": "sebEhNKdcly3",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.5031  -1.04185 -2.03875]\n",
            " [ 0.2434  -2.7332  -5.7633 ]\n",
            " [-0.99314  1.41254 -0.35655]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we are looking to add an arbitrary number of neurons and layers, we will need to create a method. the random function in numpy generates an array of random numbers of the given shape. the biases are all initialized as zeros.\n",
        "we don't initialize weights as zero because that would mean multiply inputs with zeros and we will have 0 as inputs for all neurons in the next layer. \n",
        "We also scale down our weights since these weights multiply with the inputs at every layer, the value after a few multiplications could explode as a really big number for the final layer making further calculaitons difficult."
      ],
      "metadata": {
        "id": "a_vVIIZC18rM"
      },
      "id": "a_vVIIZC18rM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e780d6ef",
      "metadata": {
        "id": "e780d6ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "586105e0-5bdd-422a-e896-d2a0963a64a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4.57089034e-02 -5.41558048e-03 -1.98420004e-02 -1.80550777e-02\n",
            "  -1.77800832e-02  6.12640653e-02 -3.69887455e-02 -8.20820222e-05\n",
            "   1.91196544e-02]\n",
            " [ 5.60240361e-02  5.66563448e-02 -6.94201882e-02 -7.57556957e-02\n",
            "  -3.17718544e-02 -9.86291349e-03 -5.49817928e-02 -3.79518315e-02\n",
            "   5.12720715e-02]\n",
            " [ 5.35053018e-02 -8.28123966e-03 -5.70026774e-02 -2.37405478e-03\n",
            "   1.84340650e-02  3.57871734e-02 -8.05422651e-03  2.85464219e-02\n",
            "   2.30146612e-02]]\n"
          ]
        }
      ],
      "source": [
        "#Generalizing for everything\n",
        "\n",
        "inputs = [[1,2,3,2.5],\n",
        "          [2.0,5.0,-1.0,2.0],\n",
        "          [-1.5,2.7,3.3,-0.8]] #The inputs will be the same \n",
        "\n",
        "class layer_dense:\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    self.weights = 0.10* np.random.randn(n_inputs,n_neurons) # we take weight like this so that we don't have to do a transpose everytime, and scale it down when we multiply 0.10\n",
        "    self.bias = np.zeros((1,n_neurons))\n",
        "  def forward(self, inputs):\n",
        "    self.output = np.dot(inputs,self.weights) + self.bias\n",
        "\n",
        "layer1 = layer_dense(4,5)\n",
        "layer2 = layer_dense(5,9)\n",
        "\n",
        "layer1.forward(inputs)\n",
        "layer2.forward(layer1.output)\n",
        "\n",
        "print(layer2.output)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Activation functions**\n",
        "\n",
        "Now let's take a look at activations functions.\n",
        "they are necessary to bring non-lineraity to our neural network. which means that it provides granularity to our output and scales it between some values.\n",
        "the outputs of every neuron is in linear form. like that of a straight line. this will make it difficult to model data that is distributed for example in a spiral way. using these activation functions helps the model to shape accroding to the data's distribution. giving higher accuracy during predictions or testing.\n",
        "so we have sigmoid function where y = 1/1+e^(-x) which rescales input between 0 and 1. then we have ReLu which gives linear function as well but if value s of x is less than 0 then it clips it at 0. this may seem linear but we only get non linearity when multiple neurons with relu work together."
      ],
      "metadata": {
        "id": "Yu45i8X74mea"
      },
      "id": "Yu45i8X74mea"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Activation_ReLu:\n",
        "  def forward(self,inputs):\n",
        "    self.output = np.maximum(0,inputs)\n",
        "\n",
        "#We will use ReLU on all of our layers except the final one"
      ],
      "metadata": {
        "id": "WEJetLN1c08X"
      },
      "id": "WEJetLN1c08X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**softmax activation for final layer**\n",
        "\n",
        "We will also take a look at the soft-max function since we cannot really use ReLU in the final layer of our network. because it will clip the negative values to 0. softmax will give us the probabilities of each one being the right answer. for example what is the probablility that a certain picture is a cat picture or a dog picture.\n",
        "first we will take the exponentitation of our output values after which we will normalize it which means the individual values from our output matrix over the sum of all the values in that matrix.\n",
        "one problem that we would come across with exponentiation is that the explosion of values as the input to the funciton grows, this could reach overflow as well.\n",
        "to prevent this we are actually going to subtract the maximum value from our outputs with our value. this will make sure the values in the exponentiation is between 0 to 1.  one concern we may have is that how will this impact the output of softmax activation funciton. once we do the normalization, we'll see that the actual output is just the same as before.\n"
      ],
      "metadata": {
        "id": "wPx2ZWS6vyDO"
      },
      "id": "wPx2ZWS6vyDO"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "\n",
        "outputs = [1.2,3.5,2.5]\n",
        "\n",
        "exponentiated = np.exp(outputs) \n",
        "normalized = exponentiated / np.sum(exponentiated) # This combination of exponentiation and normalization is called softmax\n",
        "\n",
        "class Activation_softmax:\n",
        "  def forward(self,inputs):\n",
        "    exponentiated_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True)) #if we don't provide the axis it will return the max from the whole batch of inputs\n",
        "    normalized = exponentiated_values / np.sum(exponentiated_values, axis = 1, keepdims= True) #if we don't use the two parameters we will get a single value for the sum\n",
        "    self.output = normalized\n",
        "\n",
        "\n",
        "\n",
        "print(sum(normalized)) #note that all probabilities add up to 1 "
      ],
      "metadata": {
        "id": "uWCEQ05Qcl3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8006d52d-2c96-4071-edd3-431239f6bea9"
      },
      "id": "uWCEQ05Qcl3L",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The forward pass**\n",
        "\n",
        "Now that we have all the components for the forward pass, let's piece it all together. we are still using custom inputs but we can use whatever inputs we would like to use. I have only used 2 layers for the purpose of illustration."
      ],
      "metadata": {
        "id": "iL2LCCX86wvD"
      },
      "id": "iL2LCCX86wvD"
    },
    {
      "cell_type": "code",
      "source": [
        "x = [[1,2,3,2.5],\n",
        "     [2.0,5.0,-1.0,2.0],\n",
        "     [-1.5,2.7,3.3,-0.8]]\n",
        "\n",
        "layer1 = layer_dense(4,5)\n",
        "layer1_activation = Activation_ReLu()\n",
        "\n",
        "layer2 = layer_dense(5,2)\n",
        "layer2_activation = Activation_softmax()\n",
        "\n",
        "layer1.forward(x)\n",
        "layer1_activation.forward(layer1.output)\n",
        "\n",
        "layer2.forward(layer1_activation.output)\n",
        "layer2_activation.forward(layer2.output)\n",
        "\n",
        "print(layer2_activation.output) #This is our final output\n",
        "print(np.sum(layer2_activation.output, axis=1, keepdims= True)) #Note that the sum of all the probabilities add to 1\n"
      ],
      "metadata": {
        "id": "ORaJuXVfcl7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d2961ef-4f6e-4171-950e-6b08c0dfd431"
      },
      "id": "ORaJuXVfcl7c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.55838635 0.44161365]\n",
            " [0.54539434 0.45460566]\n",
            " [0.51320077 0.48679923]]\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss function (Categorical-cross entropy)**\n",
        "\n",
        "We would now need a loss function. a loss function tells us how bad the current value for weights and biases are. we will use a loss called categorical cross entropy and we will one hot enconde our target classes(true outputs). \n",
        "example for one hot encoding is, suppose we have 3 classes then we will encode the class at first index -> [1,0,0] , second index -> [0,1,0], third index = [0,0,1] for our individual true outputs for corresponding to every entry.\n",
        "\n",
        "\n",
        "categorical cross entropy will take the negative sum, of the target value mulitplied by the log of the predicted value for each class in the distribution\n"
      ],
      "metadata": {
        "id": "Tdk6l9_jCnDW"
      },
      "id": "Tdk6l9_jCnDW"
    },
    {
      "cell_type": "code",
      "source": [
        "#let's understand with an example first, note that this is for a single input.\n",
        "\n",
        "softmax_output = [0.2,0.5,0.3] #The predicted output \n",
        "target_output = [1,0,0] #The desired output\n",
        "\n",
        "loss = -(np.log(softmax_output[0])*target_output[0] +\n",
        "         np.log(softmax_output[1])*target_output[1] +\n",
        "         np.log(softmax_output[2])*target_output[2])\n",
        "\n",
        "print(loss)\n",
        "print(-np.log(0.2)) #Both of these values are the same since we are only taking the class prediction probabilities of the correct class as the loss\n",
        "\n",
        "#Now we need to take these inputs in batches\n",
        "\n",
        "softmax_outputs = [[0.3,0.4,0.3],\n",
        "                   [0.1,0.7,0.2],\n",
        "                   [0.9,0.1,0.0]]\n",
        "\n",
        "#Let's say that the targets are cat, cat, dog where the first index in the input is the dog class and 2nd is the cat class and thrid is bird class\n",
        "#so we will have the corresponding correct distributions at the indexes [1,1,0]\n",
        "#the correct labels would then be [0.4,0.7,0.9]\n",
        "#we can do this the following way\n",
        "\n",
        "softmax_outputs = np.array([[0.3,0.4,0.3],\n",
        "                            [0.1,0.7,0.2],\n",
        "                            [0.9,0.1,0.0]])\n",
        "\n",
        "class_target = [1,1,0]\n",
        "\n",
        "\n",
        "print(softmax_outputs[[0,1,2], class_target]) # Here the first list has the list_index and the second list has the correspoding value's index\n",
        "\n",
        "\n",
        "log_loss = -np.log(softmax_outputs[range(len(softmax_outputs)),class_target])\n",
        "avg_loss = np.mean(log_loss)\n",
        "print(avg_loss)\n",
        "\n",
        "#here we are taking the range of len of softmax to generalize for the number of inputs.\n",
        "#Finally we are taking negative log of these values to get the corresponding losses for the inputs\n",
        "#And then we would take the mean of those losses\n",
        "#Sometimes we may come accross a problem where we would have a zero in our distribution, therefore the log of it would be infinite and avg of inf is inf\n",
        "#to deal with the above issue we will simply clip the values between (1e-7) which is a pretty insignificant value and (1-1e-7)\n",
        "\n",
        "\n",
        "class Loss:\n",
        "  def calculate(self, output, y):\n",
        "    sample_losses = self.forward(output,y)\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    return data_loss\n",
        "\n",
        "class Loss_CategoricalCrossEntropy(Loss):\n",
        "  def forward(self, y_pred, y_true):\n",
        "    samples = len(y_pred)\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
        "\n",
        "    if len(y_true.shape) == 1 :\n",
        "      correct_prob = y_pred_clipped[range(samples), y_true]\n",
        "    elif len(y_true.shape) == 2 :\n",
        "      correct_prob = np.sum(y_pred_clipped*y_true, axis =1)\n",
        "\n",
        "    negative_log_loss = np.log(correct_prob)\n",
        "    return negative_log_loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bQZDHLP_ueL",
        "outputId": "1fd3472e-783a-4529-d195-261f677543b2"
      },
      "id": "2bQZDHLP_ueL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6094379124341003\n",
            "1.6094379124341003\n",
            "[0.4 0.7 0.9]\n",
            "0.4594420638235713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1,2,3,2.5],\n",
        "     [2.0,5.0,-1.0,2.0],\n",
        "     [-1.5,2.7,3.3,-0.8]]) #input values\n",
        "\n",
        "y = np.array([[0,1,0],\n",
        "     [0,1,0],\n",
        "     [1,0,0]]) #Correct labels\n",
        "\n",
        "\n",
        "layer1 = layer_dense(4,5)\n",
        "layer1_activation = Activation_ReLu()\n",
        "\n",
        "layer2 = layer_dense(5,3)\n",
        "layer2_activation = Activation_softmax()\n",
        "\n",
        "layer1.forward(x)\n",
        "layer1_activation.forward(layer1.output)\n",
        "\n",
        "layer2.forward(layer1_activation.output)\n",
        "layer2_activation.forward(layer2.output)\n",
        "\n",
        "loss_function = Loss_CategoricalCrossEntropy()\n",
        "loss = loss_function.calculate(layer2_activation.output, y) #we need to make sure that the shape matches for the output of the softmax and the correct target labels\n",
        " \n",
        "\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yhpRNKuh1In",
        "outputId": "10d775b7-8a2c-487a-c36d-163592be96db"
      },
      "id": "0yhpRNKuh1In",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.1259439547329413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Backpropagation will be continued soon"
      ],
      "metadata": {
        "id": "SHLRRyRdadUe"
      },
      "id": "SHLRRyRdadUe"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}